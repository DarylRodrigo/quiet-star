{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbd5027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62c86698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.models.mistral import configuration_mistral as original_configuration_mistral\n",
    "from transformers.models.mistral import modeling_mistral as original_modeling_mistral\n",
    "\n",
    "import configuration_mistral\n",
    "import modeling_mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c93706a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0797ef004e45cbbe27fd7cf5bd18c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer\n",
      "Encoding input\n",
      "Running Generation\n",
      "Current token: 0\n",
      "Current token: 1\n",
      "Current token: 2\n",
      "Current token: 3\n",
      "Current token: 4\n",
      "Current token: 5\n",
      "Current token: 6\n",
      "Current token: 7\n",
      "Current token: 8\n",
      "Current token: 9\n",
      "Current token: 10\n",
      "Current token: 11\n",
      "Current token: 12\n",
      "Current token: 13\n",
      "Current token: 14\n",
      "Current token: 15\n",
      "Current token: 16\n",
      "Current token: 17\n",
      "Current token: 18\n",
      "Current token: 19\n",
      "Current token: 20\n",
      "Current token: 21\n",
      "Current token: 22\n",
      "Current token: 23\n",
      "Current token: 24\n",
      "Current token: 25\n",
      "Current token: 26\n",
      "Current token: 27\n",
      "Current token: 28\n",
      "Current token: 29\n",
      "Current token: 30\n",
      "Current token: 31\n",
      "Current token: 32\n",
      "Current token: 33\n",
      "Current token: 34\n",
      "Current token: 35\n",
      "Current token: 36\n",
      "Current token: 37\n",
      "Current token: 38\n",
      "Current token: 39\n",
      "Current token: 40\n",
      "Current token: 41\n",
      "Current token: 42\n",
      "Current token: 43\n",
      "Current token: 44\n",
      "Current token: 45\n",
      "Current token: 46\n",
      "Current token: 47\n",
      "Current token: 48\n",
      "Current token: 49\n",
      "Current token: 50\n",
      "Current token: 51\n",
      "Current token: 52\n",
      "Current token: 53\n",
      "Current token: 54\n",
      "Current token: 55\n",
      "Current token: 56\n",
      "Current token: 57\n",
      "Current token: 58\n",
      "Current token: 59\n",
      "Current token: 60\n",
      "Current token: 61\n",
      "Current token: 62\n",
      "Current token: 63\n",
      "Current token: 64\n",
      "Current token: 65\n",
      "Current token: 66\n",
      "Current token: 67\n",
      "Current token: 68\n",
      "Current token: 69\n",
      "Current token: 70\n",
      "Current token: 71\n",
      "Current token: 72\n",
      "Current token: 73\n",
      "Current token: 74\n",
      "Current token: 75\n",
      "Current token: 76\n",
      "Current token: 77\n",
      "Current token: 78\n",
      "Current token: 79\n",
      "Current token: 80\n",
      "Current token: 81\n",
      "Current token: 82\n",
      "Current token: 83\n",
      "Current token: 84\n",
      "Current token: 85\n",
      "Current token: 86\n",
      "Current token: 87\n",
      "Current token: 88\n",
      "Current token: 89\n",
      "Current token: 90\n",
      "Current token: 91\n",
      "Current token: 92\n",
      "Current token: 93\n",
      "Current token: 94\n",
      "Current token: 95\n",
      "Current token: 96\n",
      "Current token: 97\n",
      "Current token: 98\n",
      "Current token: 99\n",
      "<s> Solve the equation 2x + 3x² = 5.\n",
      "\n",
      "Solution\n",
      "\n",
      "We start by rearranging the terms to put all the x-terms on one side and all the constants on the other:\n",
      "\n",
      "\\[ 2x + 3x^2 - 5 = 0. \\]\n",
      "\n",
      "To solve this equation, we have to find the values of x that make the equation true.\n",
      "\n",
      "We know that if we add or subtract the same number from both sides of an equation, the resulting equation will also\n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "original_modeling_mistral.MistralModel = modeling_mistral.MistralModel\n",
    "original_modeling_mistral.MistralForCausalLM = modeling_mistral.MistralForCausalLM\n",
    "original_configuration_mistral.MistralConfig = configuration_mistral.MistralConfig\n",
    "\n",
    "model_path = \"ezelikman/quietstar-8-ahead\"\n",
    "\n",
    "n_ahead = 8\n",
    "n_ahead_talk = 1\n",
    "merged_talk_heads = True\n",
    "\n",
    "print(\"Loading model\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                             load_in_8bit=True,\n",
    "                                             max_thoughts=n_ahead + n_ahead_talk + 1,\n",
    "                                             merged_talk_heads=merged_talk_heads,\n",
    "                                             merged_lm_and_talk_heads=False,\n",
    "                                             merged_lm_and_think_heads=True,\n",
    "                                             use_concat_talk_head=True,\n",
    "                                             use_shallow_think=True,\n",
    "                                             use_shallow_talk=False,\n",
    "                                             use_complex_think_head=False,\n",
    "                                             use_complex_talk_head=True,\n",
    "                                             use_weighted_talk_head=True,\n",
    "                                             )\n",
    "\n",
    "print(\"Loading tokenizer\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "model.use_end_thought_token = True\n",
    "model.tokenizer = tokenizer\n",
    "model.use_start_thought_token = True\n",
    "model.wandb_enabled = True\n",
    "model.n_ahead = n_ahead\n",
    "model.n_passes = 1\n",
    "model.eval_mode = True\n",
    "model.first_run = False\n",
    "model.kill_after = 100\n",
    "model.rm_initialized = True\n",
    "\n",
    "model.original_mode = False\n",
    "\n",
    "input = \"Solve the equation 2x + 3x² = 5.\"\n",
    "\n",
    "print(\"Encoding input\")\n",
    "input_ids = tokenizer.encode(input, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# output = model.generate(input_ids, max_length=50)\n",
    "\n",
    "\n",
    "def generate(input_ids, attention_mask, model, temp, max_length=20):\n",
    "    with torch.no_grad():\n",
    "        finished_generating = torch.zeros(len(input_ids), dtype=torch.bool, device=input_ids.device)\n",
    "        for cur_token_idx in range(max_length):\n",
    "            print(\"Current token: {}\".format(cur_token_idx))\n",
    "            # Sample the next token\n",
    "            new_ids = model(\n",
    "                input_ids[~finished_generating],\n",
    "                attention_mask=attention_mask[~finished_generating]\n",
    "            )['logits']\n",
    "            # Mask out the start and end thought tokens so we don't accidentally sample them\n",
    "            new_ids[:, :, model.tokenizer.vocab_size:] = -float(\"inf\")\n",
    "            for list_idx, answer_idx in enumerate((~finished_generating).nonzero(as_tuple=True)[0]):\n",
    "                # Find the index of the last token that is not padding\n",
    "                base_answer_ids = input_ids[answer_idx]\n",
    "                new_answer_ids = new_ids[list_idx]\n",
    "                last_token_idx = (base_answer_ids != model.tokenizer.pad_token_id).nonzero(as_tuple=True)[0].max()\n",
    "\n",
    "\n",
    "                new_ids_sampled = torch.multinomial(\n",
    "                        torch.nn.functional.softmax(new_answer_ids[last_token_idx] / temp, dim=-1), 1)\n",
    "                # Assign the new id to the last token\n",
    "                if last_token_idx + 1 >= len(base_answer_ids):\n",
    "                    # Add padding everywhere\n",
    "                    new_padding = torch.full((len(input_ids), 1), model.tokenizer.pad_token_id, dtype=torch.long,\n",
    "                                             device=input_ids.device)\n",
    "                    input_ids = torch.cat([input_ids, new_padding], dim=-1)\n",
    "                    attention_mask = torch.cat([attention_mask, torch.zeros_like(new_padding)], dim=-1)\n",
    "                attention_mask[answer_idx, last_token_idx + 1] = 1\n",
    "                input_ids[answer_idx, last_token_idx + 1] = new_ids_sampled\n",
    "                if new_ids_sampled == model.tokenizer.eos_token_id or new_ids_sampled == model.tokenizer.bos_token_id or new_ids_sampled == model.tokenizer.pad_token_id:\n",
    "                    finished_generating[answer_idx] = 1\n",
    "            if finished_generating.all():\n",
    "                break\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "\n",
    "print(\"Running Generation\")\n",
    "out = generate(input_ids, torch.ones_like(input_ids), model, 0.9, max_length=100)\n",
    "print(tokenizer.decode(out[0][0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ae57e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
